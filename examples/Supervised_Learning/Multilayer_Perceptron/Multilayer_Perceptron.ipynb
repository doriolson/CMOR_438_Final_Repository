{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c95617a",
   "metadata": {},
   "source": [
    "Here is the model for the Multilayer Perceptron Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00305bc0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# first, load in the necessary packages and functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, classification_report)\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32e8fef",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Next, you must load in the data\n",
    "df = load_and_prepare_data(\"adult.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcc9469",
   "metadata": {},
   "source": [
    "Now we get to the actual training and evaluation of the Multilayer perceptron. 80% of the data is used for training, while the other 20% is kept separate for testing. In each epoch, the inputs are passed through the layers and given the resulting weights. Once the output is determined given our threshold, the algorithm then tests to see if the answer was correct, and propogates back through with an update if not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db3b77a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# train the MLP\n",
    "model, X_test, y_test = train_mlp(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640feb2d",
   "metadata": {},
   "source": [
    "Once the model is trained, we can now run the evaluation to see how accurate and efficient the MLP was at learning this data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9e6418",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# evaluate the MLP\n",
    "evaluate_model(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee5e279",
   "metadata": {},
   "source": [
    "## Visualizations\n",
    "We can also create an assortment of visualizations to determine the effectiveness of the model. The code for these visualizations is once again in the src file, but here we can call the functions and discuss the results.\n",
    "\n",
    "The first graph is a Principal Component Analysis (PCA) of the Projection of Learned Representation. This helps simplify the data into a format that we can understand more easily while still being able to interpret and draw findings from the data. A PCA picks two components that best influences our variable of interest (income) to see if there are patterns that can determine how to predict income. From this PCA, we can see a lot of overlaps between the income variables within the larger cluster, but there is enough of a separation between the colors in the large cluster to indicate that there is a slightly distinguishable pattern or method of determining income due to these other factors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ec8e59",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plot_pca_representation(model, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29a770d",
   "metadata": {},
   "source": [
    "The second graph is a Training Loss Model which shows the convergence of data, or how many iterations it took the algorithm to understand the data. After the first iteration, the graph shows a pretty steady linear decline in the relationship between training loss and iterations, meaning the more iterations the MLP runs, the lower the training loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24fc21f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plot_training_loss(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001ad785",
   "metadata": {},
   "source": [
    "The final graph here is a Precision-Recall Curve, which is helpful when data is imbalanced. As we saw in the PCA curve, this adult income dataset is a bit unbalanced and thus this can be helpful. This graph measures the precision of the MLP, specifically the rate of false positives and true positives. Precision measures correctly identified positives, while recall simply measures any positive recorded in the data. The graph shows an exponentially negative relation between the two, meaning that if you want high precision you will have very few cases, but if you want all cases of positives you will not be very accurate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5cce27",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plot_precision_recall(model, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
