{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ba2e6a7",
   "metadata": {},
   "source": [
    "## K Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0af14be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa99c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all of the functions from k means clustering src\n",
    "from rice_ml.k_means_clustering import (get_features, \n",
    "                                        get_feature_types, \n",
    "                                        create_preprocessor,\n",
    "                                        plot_elbow_curve,\n",
    "                                        plot_silhouette_scores,\n",
    "                                        train_kmeans,\n",
    "                                        evaluate_clustering,\n",
    "                                        plot_pca_clusters,\n",
    "                                        plot_cluster_distribution,\n",
    "                                        cluster_vs_target,\n",
    "                                        cluster_numeric_summary\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eee1490",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Load and preprocess data\n",
    "\n",
    "# load dataset\n",
    "df = pd.read_csv(\"unsupervised_ObesityDataSet_raw_and_data_sinthetic.csv\")\n",
    "\n",
    "# Prepare features\n",
    "X = get_features(df)\n",
    "\n",
    "# Feature types\n",
    "num_features, cat_features = get_feature_types()\n",
    "\n",
    "# Preprocessing\n",
    "preprocessor = create_preprocessor(num_features, cat_features)\n",
    "\n",
    "# Transform data once for evaluation plots\n",
    "X_processed = preprocessor.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9bbb7a",
   "metadata": {},
   "source": [
    "Once everything has been loaded and the data has been prepared for the algortithm, we can begin training the model. For K-means clustering, we need to start by picking the k, or number of clusters which could be done by looking at the elbow curve. From the graph, we can see that the elbow of the data is around k = 5, but it is a bit difficult to determine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5d761d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_elbow_curve(X_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58116056",
   "metadata": {},
   "source": [
    "Since we did not get a clear answer from the elbow graph, we can instead try a Silhouette Score Analysis. In this chart, a higher value means clearer defined clusters. Since this is the first iteration, the silhouette analysis shows overall weak clustering, but we can still see that 4 clusters seems to provide the most separation. Thus, we will use k = 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461f66e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_silhouette_scores(X_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58503176",
   "metadata": {},
   "source": [
    "Once K has been selected, we can begin training the model! We are running the model for 50 iterations. Each time, the model will propose an improved set of centroids (cluster centers) that more accurately sort the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb6a942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define k\n",
    "optimal_k = 4\n",
    "\n",
    "# Train final model\n",
    "pipeline, clusters = train_kmeans(preprocessor, X, optimal_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a91a82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Cluster\"] = clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89736363",
   "metadata": {},
   "source": [
    "\n",
    "Once the machine has learned, we can evaluate how well the model did. We can determine this by looking at the Silhouette score of the final clusters (0.155) and also a map of the final clusters. The map appears to have 4 distinct clusters, as demarcated by the clusters, but there is some overlap; it is hard to separate some of the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a88da36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Analysis Score\n",
    "sil_score = evaluate_clustering(X_processed, clusters)\n",
    "print(f\"Silhouette Score: {sil_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71457a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of final clusters\n",
    "plot_pca_clusters(X_processed, clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528a076c",
   "metadata": {},
   "source": [
    "In addition to the spatial map of the clusters, we can look at the distribution of variables within each cluster. First, we can see the number of observations assigned to each cluster. This can be done across the entire cluster, as in the bar chart, or broken down by variable in the next table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076be1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar Chart: distribution of observations in clusters\n",
    "plot_cluster_distribution(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dcd783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table: distribution of observation variables in clusters\n",
    "print(cluster_vs_target(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb8d2ec",
   "metadata": {},
   "source": [
    "We can also see the average values of each variable within the clusters. This is a great way to see if the observations were actually matched into the correct clusters. As we can see, there are clear differences in values in many of the variables between the clusters (especially Age, Weight, and NCP), indicating that the algorithm did its job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9f095a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cluster_numeric_summary(df, num_features))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
